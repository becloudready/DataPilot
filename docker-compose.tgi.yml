services:
  postgres:
    image: postgres:latest
    container_name: pg-database
    ports:
      - "5432:5432"  # Publicly accessible on port 5432
    networks:
      - my_network  # Internal Docker network
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql

  db-agent:
    build: .
    container_name: db-agent-app
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/postgres
      LLM_ENDPOINT: huggingface-tgi
      LLM: defog/llama-3-sqlcoder-8b
      DB_DRIVER: postgres
      DB_USER: postgres
      DB_NAME: db-agent
      DB_PASSWORD: postgres
      DB_PORT: 5432
    ports:
      - "8501:8501"  # Publicly accessible on port 8501
    networks:
      - my_network
    depends_on:
      - postgres

  data-importer:
    image: python:3.9
    container_name: data-importer
    depends_on:
      - postgres
    networks:
      - my_network
    volumes:
      - ./scripts/:/scripts  # Mount your Python script directory
    working_dir: /scripts
    entrypoint: ["sh", "-c", "sleep 10; pip3 install psycopg2-binary kagglehub; python3 data_import_lows.py; python3 data_import_sales.py"]

  huggingface-tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    container_name: text-generation-inference
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      HF_TOKEN: "${HF_TOKEN}"  # Pass the HF_TOKEN environment variable
    volumes:
      - huggingface:/data/hub/models
      - "~/.cache/huggingface:/root/.cache/huggingface"  # Cache directory for Hugging Face
    ports:
      - "8000:80"  # Expose port 8000 on the host to port 80 in the container
    command: --model-id meta-llama/Llama-3.2-3B-Instruct
    networks:
      - my_network


volumes:
  pg_data:
  huggingface:

networks:
  my_network:
    external: false
