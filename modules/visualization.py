import streamlit as st
import pandas as pd
import numpy as np
from model_backends.inference_server import InferenceServerClient


inference_server_address="198.145.127.92:8000"
model_name="defog/llama-3-sqlcoder-8b"

st.set_page_config(
    page_title="Data Visualization",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

def show_visualization():
    st.title("Data Visualization")
    st.write("This page allows you to visualize data from a CSV file.")

    # Option to upload file or enter path
    input_method = st.radio("Choose how to provide the CSV file:", ("Upload File", "Enter File Path"))

    if input_method == "Upload File":
        uploaded_file = st.file_uploader("Upload your CSV file", type="csv")
        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            display_data_and_query(df)

    elif input_method == "Enter File Path":
        file_path = st.text_input("Enter the path to your CSV file:")
        if file_path:
            try:
                df = pd.read_csv(file_path)
                display_data_and_query(df)
            except FileNotFoundError:
                st.error("File not found. Please check the file path.")

def display_data_and_query(df):
    """Display data and set up natural language querying with LLM."""
    print("call===")
    st.write("Data Preview:")
    st.write(df.head())
    prompt = st.text_area("Your query in plain English:")

    if st.button("Submit Question"):
        if prompt:
        
            with st.spinner(f"Generating response with {model_name}"):
                inference_client = InferenceServerClient(
                server_url=f"http://{inference_server_address}/v1/chat/completions",
                model_name=model_name
            )
            print(model_name,inference_server_address)
            generated_code = inference_client.query_llm_for_code(prompt,df)
            st.write(generated_code)
            st.code(generated_code)
            print(generated_code)
            try:
                # Local namespace for exec to prevent conflicts
                local_vars = {'df': df}

                # Execute the code
                exec(generated_code, {}, local_vars)

                # Display any plot generated by the code
                if 'plt' in local_vars:
                    st.pyplot(local_vars['plt'])  # Matplotlib plot
                elif 'fig' in local_vars:
                    st.plotly_chart(local_vars['fig'])  # Plotly figure

            except Exception as e:
                st.error(f"Error executing generated code: {e}")
    st.button("Wait for result")



show_visualization()


